{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '.csv'"
      ],
      "metadata": {
        "id": "JYnF1RN6q35N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "P_fnt6ztru4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EN-zZi6pHIB"
      },
      "source": [
        "### Install the Huggingface transformers module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD6iOcTmoaxE",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:12.875672Z",
          "iopub.execute_input": "2021-08-07T14:03:12.876080Z",
          "iopub.status.idle": "2021-08-07T14:03:19.269243Z",
          "shell.execute_reply.started": "2021-08-07T14:03:12.876045Z",
          "shell.execute_reply": "2021-08-07T14:03:19.268187Z"
        },
        "trusted": true
      },
      "source": [
        "! pip -q install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXuRTjrJo5vk"
      },
      "source": [
        "## Import DialoGPT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSvzC1j7_Tr8",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:19.272773Z",
          "iopub.execute_input": "2021-08-07T14:03:19.273052Z",
          "iopub.status.idle": "2021-08-07T14:03:22.767334Z",
          "shell.execute_reply.started": "2021-08-07T14:03:19.273023Z",
          "shell.execute_reply": "2021-08-07T14:03:22.766473Z"
        },
        "trusted": true
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_size = \"small\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kj2BIaUiS71"
      },
      "source": [
        "## Configuring the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv9TXRvV1HIk",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:36.771359Z",
          "iopub.execute_input": "2021-08-07T14:03:36.771854Z",
          "iopub.status.idle": "2021-08-07T14:03:36.785617Z",
          "shell.execute_reply.started": "2021-08-07T14:03:36.771820Z",
          "shell.execute_reply": "2021-08-07T14:03:36.784711Z"
        },
        "trusted": true
      },
      "source": [
        "import glob, logging, os, pickle, random, re, torch, pandas as pd, numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from pathlib import Path\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except ImportError:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.output_dir = f'output-{model_size}'\n",
        "        self.model_type = 'gpt2'\n",
        "        self.model_name_or_path = f'microsoft/DialoGPT-{model_size}'\n",
        "        self.config_name = f'microsoft/DialoGPT-{model_size}'\n",
        "        self.tokenizer_name = f'microsoft/DialoGPT-{model_size}'\n",
        "        self.cache_dir = 'cached'\n",
        "        self.block_size = 512\n",
        "        self.per_gpu_train_batch_size = 4\n",
        "        self.gradient_accumulation_steps = 1\n",
        "        self.learning_rate = 5e-5\n",
        "        self.weight_decay = 0.0\n",
        "        self.adam_epsilon = 1e-8\n",
        "        self.max_grad_norm = 1.0\n",
        "        self.num_train_epochs = 4  # 3\n",
        "        self.max_steps = -1\n",
        "        self.warmup_steps = 0\n",
        "        self.logging_steps = 1000\n",
        "        self.save_total_limit = None\n",
        "        self.seed = 42\n",
        "        self.local_rank = -1\n",
        "\n",
        "args = Args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAES6tb6jh1O"
      },
      "source": [
        "# Gather the training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFZsicHfjpxz",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:38.725439Z",
          "iopub.execute_input": "2021-08-07T14:03:38.725717Z",
          "iopub.status.idle": "2021-08-07T14:03:38.754588Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.725675Z",
          "shell.execute_reply": "2021-08-07T14:03:38.753871Z"
        },
        "trusted": true
      },
      "source": [
        "df = pd.read_csv(DATA_PATH)\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gRD9mhKkcT9"
      },
      "source": [
        "### Formatting the data and defining some helper functions\n",
        "We need to construct the data in the right format so the bot can interpret it properly. To do this we're adding special characters like the 'end of string' charater\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdjT5EqKkwZb",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:38.906722Z",
          "iopub.execute_input": "2021-08-07T14:03:38.907102Z",
          "iopub.status.idle": "2021-08-07T14:03:38.917840Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.907072Z",
          "shell.execute_reply": "2021-08-07T14:03:38.916753Z"
        },
        "trusted": true
      },
      "source": [
        "def construct_conv(row, tokenizer, eos = True):\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
        "    conv = flatten(conv)\n",
        "    return conv\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, df_trn):\n",
        "    print(\"load and cache examples is being run ****************************************\")\n",
        "    df_trn = df_trn.dropna()\n",
        "    return ConversationDataset(tokenizer, args, df_trn)\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
        "\n",
        "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
        "        directory = args.cache_dir\n",
        "        cached_features_file = os.path.join(directory, args.model_type + \"_cached_lm_\" + str(block_size))\n",
        "\n",
        "        logger.info(\"Creating features from dataset file at %s\", directory)\n",
        "        self.examples = []\n",
        "        for _, row in df.iterrows():\n",
        "            conv = construct_conv(row, tokenizer)\n",
        "            self.examples.append(conv)\n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        with open(cached_features_file, \"wb\") as handle:\n",
        "            pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item], dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83oT-xHu4msu"
      },
      "source": [
        "# Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W9ZUG-14pI_",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:38.935450Z",
          "iopub.execute_input": "2021-08-07T14:03:38.935954Z",
          "iopub.status.idle": "2021-08-07T14:03:38.969882Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.935882Z",
          "shell.execute_reply": "2021-08-07T14:03:38.969106Z"
        },
        "trusted": true
      },
      "source": [
        "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
        "    )\n",
        "\n",
        "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "    model = model.module if hasattr(model, \"module\") else model\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    logger.info(\"*** Running trainng, Num examples = %d, Num Epochs = %d ***\", len(train_dataset), args.num_train_epochs)\n",
        "\n",
        "    global_step, epochs_trained = 0, 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
        "    )\n",
        "    set_seed(args)  # Added here for reproducibility\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            inputs, labels = (batch, batch)\n",
        "            if inputs.shape[1] > 1024: continue\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "    tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWjTu6fI4yP8"
      },
      "source": [
        "# Main Runner\n",
        "\n",
        "Here we're simply setting up the logger and starting the training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jludg4aN4zdc",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:38.971342Z",
          "iopub.execute_input": "2021-08-07T14:03:38.971807Z",
          "iopub.status.idle": "2021-08-07T14:03:38.987706Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.971770Z",
          "shell.execute_reply": "2021-08-07T14:03:38.986681Z"
        },
        "trusted": true
      },
      "source": [
        "def main(df_trn):\n",
        "    args = Args()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    device = torch.device(\"cuda\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s\", args.local_rank, device, args.n_gpu)\n",
        "\n",
        "    set_seed(args) # Set seed\n",
        "\n",
        "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
        "    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, from_tf=False, config=config, cache_dir=args.cache_dir)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Training\n",
        "    train_dataset = load_and_cache_examples(args, tokenizer, df_trn)\n",
        "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "    model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "    # Good practice: save your training arguments together with the trained model\n",
        "    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "    # Load a trained model and vocabulary that you have fine-tuned\n",
        "    model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
        "    model.to(args.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfTdpQy-5D1n",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:38.989335Z",
          "iopub.execute_input": "2021-08-07T14:03:38.989811Z",
          "iopub.status.idle": "2021-08-07T14:08:00.933974Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.989772Z",
          "shell.execute_reply": "2021-08-07T14:08:00.933072Z"
        },
        "trusted": true
      },
      "source": [
        "\n",
        "main(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkZ0yjsc5LX-",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:25:59.805297Z",
          "iopub.execute_input": "2021-08-07T14:25:59.805625Z",
          "iopub.status.idle": "2021-08-07T14:26:35.558955Z",
          "shell.execute_reply.started": "2021-08-07T14:25:59.805593Z",
          "shell.execute_reply": "2021-08-07T14:26:35.557929Z"
        },
        "trusted": true
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(f'microsoft/DialoGPT-{model_size}')\n",
        "model = AutoModelForCausalLM.from_pretrained(f'output-{model_size}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZVKRKULw1OM"
      },
      "source": [
        "# Uploading to HuggingFace 🤗\n",
        "\n",
        "HuggingFace is a platform for hosting machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVnt8HGS3XwY",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:35.560456Z",
          "iopub.execute_input": "2021-08-07T14:26:35.560818Z",
          "iopub.status.idle": "2021-08-07T14:26:36.281001Z",
          "shell.execute_reply.started": "2021-08-07T14:26:35.560779Z",
          "shell.execute_reply": "2021-08-07T14:26:36.279937Z"
        },
        "trusted": true
      },
      "source": [
        "!sudo apt-get install git-lfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ifVP6WroFib"
      },
      "source": [
        "Git needs an email address:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl21ldCcnDIy",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:36.284686Z",
          "iopub.execute_input": "2021-08-07T14:26:36.284984Z",
          "iopub.status.idle": "2021-08-07T14:26:48.292840Z",
          "shell.execute_reply.started": "2021-08-07T14:26:36.284948Z",
          "shell.execute_reply": "2021-08-07T14:26:48.290940Z"
        },
        "trusted": true
      },
      "source": [
        "userEmail = input(\"Enter git email: \")\n",
        "!git config --global user.email \"$userEmail\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y50Xqym82bx8"
      },
      "source": [
        "Login with your huggingface account, if you don't have one you can sign up [here](https://huggingface.co/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z4uYXxO2U9W",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.294269Z",
          "iopub.status.idle": "2021-08-07T14:26:48.294929Z"
        },
        "trusted": true
      },
      "source": [
        "!huggingface-cli login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT9O8EkD2svv",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.296320Z",
          "iopub.status.idle": "2021-08-07T14:26:48.296950Z"
        },
        "trusted": true
      },
      "source": [
        "model_name = \"DiscordChatBot\"\n",
        "conversational_tag = \"\"\"---\n",
        "tags:\n",
        "- conversational\n",
        "\"\"\"\n",
        "\n",
        "model.push_to_hub(model_name)\n",
        "! echo \"$conversational_tag\" > \"$model_name/README.md\"\n",
        "tokenizer.push_to_hub(model_name)\n",
        "\n",
        "! rm -r \"$model_name/\"   # clean up local directory"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}